{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from einops import rearrange\n",
    "from tqdm import tqdm\n",
    "import SimpleITK as sitk\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "from torchsummary import summary\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torchmetrics.classification import Accuracy, AUROC, Precision, Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, img_shape=(128, 128, 128)):\n",
    "        self.df = dataframe\n",
    "        self.image_paths = self.df['ADNI_path'].values\n",
    "        self.labels = self.df['Group'].values\n",
    "\n",
    "        # Binary classification mapping\n",
    "        self.label_names = {'CN': 0, 'AD': 1, \"MCI\": 2}\n",
    "        self.num_classes = len(self.label_names)\n",
    "        self.labels_binary = self.df['Group'].map(self.label_names).values\n",
    "        \n",
    "        print(f\"{len(self.image_paths)} Images found with classification.\")\n",
    "        print(f\"Samples per class: {dict(zip(*np.unique(self.labels, return_counts=True)))}\")\n",
    "        \n",
    "        self.img_shape = img_shape\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def resize_image(self, image_np):\n",
    "        \"\"\" Resize the numpy image using scipy.ndimage.zoom \"\"\"\n",
    "        from scipy.ndimage import zoom\n",
    "        scale_factors = [n/o for n, o in zip(self.img_shape, image_np.shape)]\n",
    "        resized_image_np = zoom(image_np, scale_factors, order=1)  # Linear interpolation\n",
    "        return resized_image_np\n",
    "    \n",
    "    def process_image(self, idx):\n",
    "        try:\n",
    "            image_path = self.image_paths[idx]\n",
    "            label = self.labels_binary[idx]\n",
    "\n",
    "            image_tensor = torch.load(image_path)\n",
    "\n",
    "            if image_tensor.shape!= (1, 128, 128,128):\n",
    "                image_tensor  = image_tensor.unsqueeze(0)\n",
    "\n",
    "            return image_tensor, label\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Exception in processing image {image_path}: {e}\")\n",
    "            return None, None\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, lbl = self.process_image(idx)\n",
    "        return img, lbl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train.to_csv(\"./df_train.csv\"), df_test.to_csv(\"./df_test.csv\"), df_val.to_csv(\"./df_val.csv\")\n",
    "\n",
    "df_train = pd.read_csv(\"./df_train.csv\")\n",
    "df_val = pd.read_csv(\"./df_val.csv\")\n",
    "df_test = pd.read_csv(\"./df_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = 16\n",
    "\n",
    "img_size = (128, 128, 128, 1)\n",
    "\n",
    "train_dataset = CustomDataset(df_train, img_shape=img_size)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch, shuffle=True)\n",
    "\n",
    "val_dataset = CustomDataset(df_val, img_shape=img_size)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch, shuffle=True)\n",
    "\n",
    "test_dataset = CustomDataset(df_test, img_shape=img_size)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from ipywidgets import interact\n",
    "import numpy as np\n",
    "\n",
    "# Get one batch\n",
    "for images, labels in train_dataloader:\n",
    "    print(\"Images Shape of 1 batch:\", images.shape)  # [batch_size, 1, 128, 128, 128]\n",
    "    print(\"Labels Shape:\", labels.shape)  # [batch_size, num_classes] (one-hot encoded)\n",
    "    break  # Take only the first batch\n",
    "\n",
    "# Convert PyTorch tensor to NumPy\n",
    "img = images.numpy()  # Shape: [batch, 1, depth, height, width]\n",
    "index = 1  # Change index to visualize different samples\n",
    "\n",
    "@interact(axial_slice=(0, img.shape[3] - 1))  # Fix: Use height dimension for slicing\n",
    "def axial_slicer(axial_slice=0):\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    ax.imshow(img[index, 0, :, :, axial_slice], cmap='gray')  # Fix: Correct slicing order\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "import math\n",
    "\n",
    "def validate_config(config):\n",
    "    \"\"\"Validate configuration parameters.\"\"\"\n",
    "    required_keys = [\n",
    "        \"image_size\", \"num_channels\", \"num_classes\", \"hidden_size\", \n",
    "        \"num_hidden_layers\", \"num_attention_heads\", \"intermediate_size\"\n",
    "    ]\n",
    "    \n",
    "    for key in required_keys:\n",
    "        if key not in config:\n",
    "            raise ValueError(f\"Missing required config key: {key}\")\n",
    "    \n",
    "    # Validate dimensions\n",
    "    if config[\"hidden_size\"] % config[\"num_attention_heads\"] != 0:\n",
    "        raise ValueError(\"hidden_size must be divisible by num_attention_heads\")\n",
    "    \n",
    "    if config[\"image_size\"] <= 0 or not isinstance(config[\"image_size\"], int):\n",
    "        raise ValueError(\"image_size must be a positive integer\")\n",
    "    \n",
    "    if config[\"num_classes\"] <= 0:\n",
    "        raise ValueError(\"num_classes must be positive\")\n",
    "\n",
    "def validate_input_tensor(x, expected_channels, expected_size):\n",
    "    \"\"\"Validate input tensor dimensions.\"\"\"\n",
    "    if x.dim() != 5:\n",
    "        raise ValueError(f\"Expected 5D tensor (B, C, D, H, W), got {x.dim()}D\")\n",
    "    \n",
    "    if x.size(1) != expected_channels:\n",
    "        raise ValueError(f\"Expected {expected_channels} channels, got {x.size(1)}\")\n",
    "    \n",
    "    if x.size(2) != expected_size or x.size(3) != expected_size or x.size(4) != expected_size:\n",
    "        raise ValueError(f\"Expected size {expected_size}x{expected_size}x{expected_size}, got {x.size(2)}x{x.size(3)}x{x.size(4)}\")\n",
    "\n",
    "# --- Improved Activations ---\n",
    "class NewGELUActivation(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).\n",
    "    More stable than standard GELU with better numerical precision.\n",
    "    \"\"\"\n",
    "    def forward(self, input):\n",
    "        return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))\n",
    "\n",
    "# --- DropPath (Stochastic Depth) ---\n",
    "class DropPath(nn.Module):\n",
    "    def __init__(self, drop_prob=0.0):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.drop_prob == 0. or not self.training:\n",
    "            return x\n",
    "        keep_prob = 1 - self.drop_prob\n",
    "        shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n",
    "        random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "        random_tensor.floor_()\n",
    "        return x.div(keep_prob) * random_tensor\n",
    "\n",
    "# --- Focal Loss ---\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1.0, gamma=2.0, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
    "        return loss.mean() if self.reduction == 'mean' else loss.sum()\n",
    "\n",
    "# --- Improved ConvBlock ---\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, pool_size=2):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv3d(in_channels, out_channels, kernel_size, stride, padding),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.MaxPool3d(kernel_size=pool_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "# --- Enhanced Patch Embedding ---\n",
    "class PatchEmbedding3D(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        in_channels = config[\"num_channels\"]\n",
    "        hidden_dim = config[\"hidden_size\"]\n",
    "        channels = config[\"conv_channels\"]\n",
    "        \n",
    "        self.stages = nn.ModuleList([\n",
    "            ConvBlock(in_channels, channels[0]),\n",
    "            ConvBlock(channels[0], channels[1]),\n",
    "            ConvBlock(channels[1], channels[2]),\n",
    "            ConvBlock(channels[2], channels[3]),\n",
    "            ConvBlock(channels[3], channels[4]),\n",
    "            ConvBlock(channels[4], channels[5]),\n",
    "            ConvBlock(channels[5], hidden_dim),\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = []\n",
    "        for stage in self.stages:\n",
    "            x = stage(x)\n",
    "            features.append(x)\n",
    "        return features[-5:]  # return last 5 feature maps for BiFPN\n",
    "\n",
    "# --- Fixed BiFPN Block ---\n",
    "class BiFPNBlock3D(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        in_channels_list = config[\"bifpn_channels\"]\n",
    "        out_channels = config[\"hidden_size\"]\n",
    "        \n",
    "        self.projections = nn.ModuleList([\n",
    "            nn.Conv3d(c, out_channels, kernel_size=1) for c in in_channels_list\n",
    "        ])\n",
    "        self.fusions = nn.ModuleList([\n",
    "            nn.Conv3d(out_channels, out_channels, kernel_size=3, padding=1) for _ in in_channels_list\n",
    "        ])\n",
    "\n",
    "    def forward(self, features):\n",
    "        x = [proj(f) for proj, f in zip(self.projections, features)]\n",
    "        \n",
    "        # Top-down pathway\n",
    "        for i in range(len(x) - 2, -1, -1):\n",
    "            upsampled = F.interpolate(x[i+1], size=x[i].shape[2:], mode='trilinear', align_corners=False)\n",
    "            x[i] = x[i] + upsampled\n",
    "        \n",
    "        # Bottom-up pathway - FIXED: Use adaptive pooling to match sizes\n",
    "        for i in range(1, len(x)):\n",
    "            pooled = F.adaptive_avg_pool3d(x[i-1], x[i].shape[2:])\n",
    "            x[i] = x[i] + pooled\n",
    "        \n",
    "        # Fuse features\n",
    "        fused = [fusion(f) for fusion, f in zip(self.fusions, x)]\n",
    "        return fused[-1]\n",
    "\n",
    "# --- Fast Multi-Head Attention with Flash Attention ---\n",
    "class FasterMultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Optimized multi-head attention with merged QKV projections and Flash Attention support.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config[\"hidden_size\"]\n",
    "        self.num_attention_heads = config[\"num_attention_heads\"]\n",
    "        self.attention_head_size = self.hidden_size // self.num_attention_heads\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "        self.qkv_bias = config.get(\"qkv_bias\", True)\n",
    "        \n",
    "        # Single linear layer for query, key, and value projections\n",
    "        self.qkv_projection = nn.Linear(self.hidden_size, self.all_head_size * 3, bias=self.qkv_bias)\n",
    "        self.attn_dropout = nn.Dropout(config[\"attention_probs_dropout_prob\"])\n",
    "        \n",
    "        # Output projection\n",
    "        self.output_projection = nn.Linear(self.all_head_size, self.hidden_size)\n",
    "        self.output_dropout = nn.Dropout(config[\"hidden_dropout_prob\"])\n",
    "\n",
    "    def forward(self, x, output_attentions=False):\n",
    "        batch_size, sequence_length, _ = x.size()\n",
    "        \n",
    "        # Project query, key, and value in one operation\n",
    "        qkv = self.qkv_projection(x)\n",
    "        \n",
    "        # Split into query, key, and value\n",
    "        query, key, value = torch.chunk(qkv, 3, dim=-1)\n",
    "        \n",
    "        # Reshape to (batch_size, num_heads, seq_length, head_size)\n",
    "        query = query.view(batch_size, sequence_length, self.num_attention_heads, \n",
    "                          self.attention_head_size).transpose(1, 2)\n",
    "        key = key.view(batch_size, sequence_length, self.num_attention_heads, \n",
    "                      self.attention_head_size).transpose(1, 2)\n",
    "        value = value.view(batch_size, sequence_length, self.num_attention_heads, \n",
    "                          self.attention_head_size).transpose(1, 2)\n",
    "        \n",
    "        # Flash Attention drop-in replacement\n",
    "        if hasattr(F, 'scaled_dot_product_attention') and not output_attentions:\n",
    "            # Use PyTorch 2.0+ Flash Attention (automatic memory optimization)\n",
    "            attention_output = F.scaled_dot_product_attention(\n",
    "                query, key, value, \n",
    "                dropout_p=self.attn_dropout.p if self.training else 0.0\n",
    "            )\n",
    "            attention_probs = None\n",
    "        else:\n",
    "            # Fallback to standard attention for compatibility or when attention weights needed\n",
    "            attention_scores = torch.matmul(query, key.transpose(-1, -2))\n",
    "            attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "            attention_probs = F.softmax(attention_scores, dim=-1)\n",
    "            attention_probs = self.attn_dropout(attention_probs)\n",
    "            attention_output = torch.matmul(attention_probs, value)\n",
    "        \n",
    "        # Reshape back to (batch_size, seq_length, hidden_size)\n",
    "        attention_output = attention_output.transpose(1, 2).contiguous().view(\n",
    "            batch_size, sequence_length, self.all_head_size)\n",
    "        \n",
    "        # Final projection\n",
    "        attention_output = self.output_projection(attention_output)\n",
    "        attention_output = self.output_dropout(attention_output)\n",
    "        \n",
    "        return (attention_output, attention_probs)\n",
    "\n",
    "# --- Enhanced Transformer Block ---\n",
    "class ImprovedTransformerBlock(nn.Module):\n",
    "    def __init__(self, config, drop_path=0.0):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        hidden_size = config[\"hidden_size\"]\n",
    "        mlp_dim = config[\"intermediate_size\"]\n",
    "        dropout = config[\"hidden_dropout_prob\"]\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(hidden_size)\n",
    "        self.attn = FasterMultiHeadAttention(config)\n",
    "        self.drop_path = DropPath(drop_path)\n",
    "        self.norm2 = nn.LayerNorm(hidden_size)\n",
    "        \n",
    "        # Improved MLP with NewGELU\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_size, mlp_dim),\n",
    "            NewGELUActivation(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(mlp_dim, hidden_size),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, output_attentions=False):\n",
    "        # Self-attention with residual connection and drop path\n",
    "        attn_output, attention_probs = self.attn(self.norm1(x), output_attentions=output_attentions)\n",
    "        x = x + self.drop_path(attn_output)\n",
    "        \n",
    "        # Feed-forward with residual connection and drop path\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        \n",
    "        return (x, attention_probs)\n",
    "\n",
    "# --- Main Enhanced Unified 3D ViT Model ---\n",
    "class BiFPN3DViT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Validate configuration\n",
    "        validate_config(config)\n",
    "        self.config = config\n",
    "        \n",
    "        # Extract configuration parameters\n",
    "        self.img_size = config[\"image_size\"]\n",
    "        self.num_channels = config[\"num_channels\"]\n",
    "        self.hidden_size = config[\"hidden_size\"]\n",
    "        self.num_classes = config[\"num_classes\"]\n",
    "        self.num_layers = config[\"num_hidden_layers\"]\n",
    "        \n",
    "        # Enhanced components\n",
    "        self.embedding = PatchEmbedding3D(config)\n",
    "        self.bifpn = BiFPNBlock3D(config)\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, self.hidden_size))\n",
    "        \n",
    "        # Calculate number of patches dynamically\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, self.num_channels, self.img_size, self.img_size, self.img_size)\n",
    "            dummy_features = self.embedding(dummy)\n",
    "            dummy_fused = self.bifpn(dummy_features)\n",
    "            num_patches = rearrange(dummy_fused, 'b c d h w -> b (d h w) c').shape[1]\n",
    "        \n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, self.hidden_size))\n",
    "        self.pos_drop = nn.Dropout(p=config[\"hidden_dropout_prob\"])\n",
    "        \n",
    "        # Enhanced transformer with stochastic depth\n",
    "        stochastic_depth = config.get(\"stochastic_depth\", 0.0)\n",
    "        dpr = [x.item() for x in torch.linspace(0, stochastic_depth, self.num_layers)]\n",
    "        self.transformer = nn.ModuleList([\n",
    "            ImprovedTransformerBlock(config, drop_path=dpr[i])\n",
    "            for i in range(self.num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.norm = nn.LayerNorm(self.hidden_size)\n",
    "        self.attn_pool = nn.Linear(self.hidden_size, 1)\n",
    "        \n",
    "        # Enhanced classifier\n",
    "        classifier_hidden = config.get(\"classifier_hidden\", config[\"intermediate_size\"])\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.hidden_size * 2, classifier_hidden),\n",
    "            NewGELUActivation(),\n",
    "            nn.Dropout(config[\"hidden_dropout_prob\"]),\n",
    "            nn.Linear(classifier_hidden, self.num_classes)\n",
    "        )\n",
    "        \n",
    "        # Focal loss parameters\n",
    "        focal_alpha = config.get(\"focal_alpha\", 1.0)\n",
    "        focal_gamma = config.get(\"focal_gamma\", 2.0)\n",
    "        self.criterion = FocalLoss(alpha=focal_alpha, gamma=focal_gamma)\n",
    "        \n",
    "        # Apply proper weight initialization\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def forward(self, x, output_attentions=False):\n",
    "        # Input validation\n",
    "        validate_input_tensor(x, self.num_channels, self.img_size)\n",
    "        \n",
    "        # Extract and fuse features\n",
    "        feats = self.embedding(x)\n",
    "        fused = self.bifpn(feats)\n",
    "        x = rearrange(fused, 'b c d h w -> b (d h w) c')\n",
    "        \n",
    "        # Add CLS token and positional embeddings\n",
    "        cls_token = self.cls_token.expand(x.size(0), -1, -1)\n",
    "        x = torch.cat((cls_token, x), dim=1)\n",
    "        x = self.pos_drop(x + self.pos_embed[:, :x.size(1)])\n",
    "        \n",
    "        # Process through enhanced transformer\n",
    "        all_attentions = []\n",
    "        for transformer_block in self.transformer:\n",
    "            x, attention_probs = transformer_block(x, output_attentions=output_attentions)\n",
    "            if output_attentions:\n",
    "                all_attentions.append(attention_probs)\n",
    "        \n",
    "        x = self.norm(x)\n",
    "        \n",
    "        # Dual representation: CLS token + attention pooling\n",
    "        cls_out = x[:, 0]\n",
    "        patch_tokens = x[:, 1:]\n",
    "        weights = torch.softmax(self.attn_pool(patch_tokens), dim=1)\n",
    "        patch_summary = torch.sum(weights * patch_tokens, dim=1)\n",
    "        \n",
    "        # Combine representations and classify\n",
    "        combined = torch.cat([cls_out, patch_summary], dim=-1)\n",
    "        logits = self.classifier(combined)\n",
    "        \n",
    "        if not output_attentions:\n",
    "            return (logits, None)\n",
    "        else:\n",
    "            return (logits, all_attentions)\n",
    "\n",
    "    def compute_loss(self, logits, targets):\n",
    "        \"\"\"Compute the focal loss for handling class imbalance.\"\"\"\n",
    "        return self.criterion(logits, targets)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"Proper weight initialization for better convergence and stability.\"\"\"\n",
    "        initializer_range = self.config.get(\"initializer_range\", 0.02)\n",
    "        \n",
    "        if isinstance(module, (nn.Linear, nn.Conv3d)):\n",
    "            torch.nn.init.trunc_normal_(module.weight, mean=0.0, std=initializer_range)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "        elif isinstance(module, nn.BatchNorm3d):\n",
    "            module.weight.data.fill_(1.0)\n",
    "            module.bias.data.zero_()\n",
    "        \n",
    "        # Special initialization for embeddings\n",
    "        if hasattr(module, 'pos_embed') and module.pos_embed is not None:\n",
    "            torch.nn.init.trunc_normal_(module.pos_embed, mean=0.0, std=initializer_range)\n",
    "        if hasattr(module, 'cls_token') and module.cls_token is not None:\n",
    "            torch.nn.init.trunc_normal_(module.cls_token, mean=0.0, std=initializer_range)\n",
    "\n",
    "def count_parameters(model):\n",
    "    \"\"\"Count trainable parameters in the model.\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def print_model_summary(model):\n",
    "    \"\"\"Print model architecture and parameter count.\"\"\"\n",
    "    print(f\"Enhanced BiFPN3DViT Model Summary\")\n",
    "    print(f\"=\" * 50)\n",
    "    print(f\"Total trainable parameters: {count_parameters(model):,}\")\n",
    "    print(f\"Model configuration: {model.config}\")\n",
    "\n",
    "# --- Enhanced Configuration ---\n",
    "enhanced_config = {\n",
    "    # Basic model parameters\n",
    "    \"image_size\": 128,\n",
    "    \"num_channels\": 1,\n",
    "    \"num_classes\": 3,\n",
    "    \"hidden_size\": 384,\n",
    "    \"num_hidden_layers\": 8,\n",
    "    \"num_attention_heads\": 8,\n",
    "    \"intermediate_size\": 768,\n",
    "    \n",
    "    # Dropout and regularization\n",
    "    \"hidden_dropout_prob\": 0.4,\n",
    "    \"attention_probs_dropout_prob\": 0.4,\n",
    "    \"stochastic_depth\": 0.4,\n",
    "    \n",
    "    # Convolutional feature extraction\n",
    "    \"conv_channels\": [16, 32, 64, 128, 256, 512],\n",
    "    \"bifpn_channels\": [64, 128, 256, 512, 384],\n",
    "    \n",
    "    # Loss function parameters\n",
    "    \"focal_alpha\": 1.0,\n",
    "    \"focal_gamma\": 2.0,\n",
    "    \n",
    "    # Training parameters\n",
    "    \"initializer_range\": 0.02,\n",
    "    \"qkv_bias\": True,\n",
    "    \"classifier_hidden\": 768,\n",
    "    \n",
    "    \"model_name\": \"BiFPN3DViT\",\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create enhanced model instance\n",
    "model = BiFPN3DViT(enhanced_config)\n",
    "\n",
    "# Move to appropriate device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Print model summary\n",
    "print_model_summary(model)\n",
    "\n",
    "# Test forward pass\n",
    "print(f\"\\n\" + \"=\"*50)\n",
    "print(\"Testing Forward Pass\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "batch_size = 2\n",
    "input_tensor = torch.randn(batch_size, 1, 128, 128, 128).to(device)\n",
    "sample_labels = torch.tensor([0, 1]).to(device)\n",
    "\n",
    "# Test without attention output\n",
    "logits, _ = model(input_tensor, output_attentions=False)\n",
    "print(f\"Input shape: {input_tensor.shape}\")\n",
    "print(f\"Output shape: {logits.shape}\")\n",
    "\n",
    "# Test loss computation\n",
    "loss = model.compute_loss(logits, sample_labels)\n",
    "print(f\"Loss value: {loss.item():.4f}\")\n",
    "\n",
    "# Test with attention output for interpretability\n",
    "logits_with_attn, all_attentions = model(input_tensor, output_attentions=True)\n",
    "print(f\"Number of attention layers: {len(all_attentions) if all_attentions else 0}\")\n",
    "if all_attentions:\n",
    "    print(f\"Attention shape per layer: {all_attentions[0].shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def print_model_summary(model):\n",
    "    print(model)\n",
    "    print(f\"\\nTrainable parameters: {count_parameters(model):,}\")\n",
    "\n",
    "# Assuming your model is already defined and on the correct device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Print the model summary\n",
    "print_model_summary(model)\n",
    "\n",
    "# If you want to test the forward pass\n",
    "input_tensor = torch.randn(3, 1, 128, 128, 128).to(device)  # Batch size of 3\n",
    "output = model(input_tensor)\n",
    "print(f\"\\nInput shape: {input_tensor.shape}\")\n",
    "print(f\"Output shape: {output[0].shape}\")  # Assuming the first element is the logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the optimizer (SGD with momentum)\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=1e-4,\n",
    "    weight_decay=1e-4\n",
    ")\n",
    "\n",
    "num_classes = 3\n",
    "\n",
    "# Define evaluation metrics\n",
    "accuracy = Accuracy(task=\"multiclass\", num_classes=num_classes).to(device)\n",
    "auc = AUROC(task=\"multiclass\", num_classes=num_classes).to(device)\n",
    "precision = Precision(task=\"multiclass\", num_classes=num_classes).to(device)\n",
    "recall = Recall(task=\"multiclass\", num_classes=num_classes).to(device)\n",
    "\n",
    "# Learning Rate Scheduler (Reduce LR on Plateau)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "    optimizer, T_0=10, T_mult=2, eta_min=1e-6\n",
    ")\n",
    "best_val_acc = 0.0  # Initialize best validation accuracy\n",
    "\n",
    "def save_checkpoint(model, epoch, val_acc, path=\"./weights_finalized/best_CNN_attn_ViT_BiFPN.pth\"):\n",
    "    global best_val_acc\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "\n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "        torch.save(model.state_dict(), path)\n",
    "        print(f\"âœ” Model saved at epoch {epoch} with val_acc: {val_acc:.4f}\")\n",
    "\n",
    "\n",
    "def compute_metrics(outputs, labels):\n",
    "    # Extract logits from model output tuple\n",
    "    logits = outputs[0] if isinstance(outputs, tuple) else outputs\n",
    "    \n",
    "    preds = torch.argmax(logits, dim=1)\n",
    "    acc = accuracy(preds, labels)\n",
    "    prec = precision(preds, labels)\n",
    "    reca = recall(preds, labels)\n",
    "    \n",
    "    # Handle potential NaN values\n",
    "    prec = torch.nan_to_num(prec, nan=0.0)\n",
    "    reca = torch.nan_to_num(reca, nan=0.0)\n",
    "    \n",
    "    # Compute AUC only if there are at least two classes present\n",
    "    unique_labels = torch.unique(labels)\n",
    "    if len(unique_labels) > 1:\n",
    "        aAuc = auc(logits, labels)\n",
    "    else:\n",
    "        aAuc = torch.tensor(0.0).to(device)\n",
    "\n",
    "    return acc, prec, reca, aAuc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = {\"loss\":[], \"acc\":[], \"auc\":[], \"precision\":[], \"recall\":[], \"val_loss\":[], \"val_acc\": [], \"val_auc\":[], \"val_precision\":[], \"val_recall\":[]}\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_acc, running_prec, running_reca, running_auc = 0.0, 0.0, 0.0, 0.0\n",
    "    train_loader = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{epochs} - Training\")\n",
    "    for batch in train_loader:\n",
    "        images, labels = batch\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        # Extract logits from model output tuple\n",
    "        logits = outputs[0] if isinstance(outputs, tuple) else outputs\n",
    "        loss = model.compute_loss(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        acc, prec, reca, aAuc = compute_metrics(outputs, labels)\n",
    "        running_acc += acc.item()\n",
    "        running_prec += prec.item()\n",
    "        running_reca += reca.item()\n",
    "        running_auc += aAuc.item()\n",
    "        train_loader.set_postfix(loss=running_loss/(train_loader.n+1), acc=running_acc/(train_loader.n+1))\n",
    "\n",
    "    \n",
    "    running_loss /= len(train_dataloader)\n",
    "    running_acc /= len(train_dataloader)\n",
    "    running_prec /= len(train_dataloader)\n",
    "    running_reca /= len(train_dataloader)\n",
    "    running_auc /= len(train_dataloader)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_acc, val_loss, val_prec, val_reca, val_auc = 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "    val_loader = tqdm(val_dataloader, desc=f\"Epoch {epoch+1}/{epochs} - Validation\")\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            images, labels = batch\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            # Extract logits from model output tuple\n",
    "            logits = outputs[0] if isinstance(outputs, tuple) else outputs\n",
    "            loss =  model.compute_loss(logits, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Compute metrics\n",
    "            acc, prec, reca, aAuc = compute_metrics(outputs, labels)\n",
    "            val_acc += acc.item()\n",
    "            val_prec += prec.item()\n",
    "            val_reca += reca.item()\n",
    "            val_auc += aAuc.item()\n",
    "            val_loader.set_postfix(val_loss=val_loss/(val_loader.n+1), val_acc=val_acc/(val_loader.n+1))\n",
    "\n",
    "    val_loss /= len(val_dataloader)\n",
    "    val_acc /= len(val_dataloader)\n",
    "    val_prec /= len(val_dataloader)\n",
    "    val_reca /= len(val_dataloader)\n",
    "    val_auc /= len(val_dataloader)\n",
    "\n",
    "    # Update history\n",
    "    history[\"loss\"].append(running_loss)\n",
    "    history[\"acc\"].append(running_acc)\n",
    "    history[\"precision\"].append(running_prec)\n",
    "    history[\"recall\"].append(running_reca)\n",
    "    history[\"auc\"].append(running_auc)\n",
    "    history[\"val_loss\"].append(val_loss)\n",
    "    history[\"val_acc\"].append(val_acc)\n",
    "    history[\"val_precision\"].append(val_prec)\n",
    "    history[\"val_recall\"].append(val_reca)\n",
    "    history[\"val_auc\"].append(val_auc)\n",
    "\n",
    "    # Checkpoint the best model\n",
    "    save_checkpoint(model, epoch, val_acc)\n",
    "\n",
    "    # Reduce LR on plateau\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Loss: {running_loss:.4f} - Acc: {running_acc:.4f} - Precision: {running_prec:.4f} - Recall: {running_reca:.4f} - AUC: {running_auc:.4f} - Val Loss: {val_loss:.4f} - Val Acc: {val_acc:.4f} - Val Precision: {val_prec:.4f} - Val Recall: {val_reca:.4f} - Val AUC: {val_auc:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "model.to(device)\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "# Initialize metrics\n",
    "test_loss = 0.0\n",
    "test_acc = 0.0\n",
    "test_prec = 0.0\n",
    "test_reca = 0.0\n",
    "test_auc = 0.0\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "test_dataset = CustomDataset(df_test, img_shape=img_size)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "with torch.no_grad():  # No gradient calculation needed\n",
    "    for images, labels in tqdm(test_dataloader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        # Extract logits from model output tuple\n",
    "        logits = outputs[0] if isinstance(outputs, tuple) else outputs\n",
    "        loss = model.compute_loss(logits, labels)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        # Get predictions\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        # Compute metrics\n",
    "        acc, prec, reca, aAuc = compute_metrics(outputs, labels)\n",
    "        test_acc += acc.item()\n",
    "        test_prec += prec.item()\n",
    "        test_reca += reca.item()\n",
    "        test_auc += aAuc.item()\n",
    "\n",
    "# Calculate average metrics\n",
    "test_loss /= len(test_dataloader)\n",
    "test_acc /= len(test_dataloader)\n",
    "test_prec /= len(test_dataloader)\n",
    "test_reca /= len(test_dataloader)\n",
    "test_auc /= len(test_dataloader)\n",
    "\n",
    "# Print results\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"Test Precision: {test_prec:.4f}\")\n",
    "print(f\"Test Recall: {test_reca:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_labels, all_preds))\n",
    "\n",
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming `train_acc_history` and `val_acc_history` store accuracy per epoch\n",
    "plt.plot(history[\"acc\"], 'b', label=\"Train Accuracy\")\n",
    "plt.plot(history[\"val_acc\"], 'g', label=\"Validation Accuracy\")\n",
    "\n",
    "# Plot settings\n",
    "plt.title(\"Model Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `train_acc_history` and `val_acc_history` store accuracy per epoch\n",
    "plt.plot(history[\"auc\"], 'b', label=\"Train AUC\")\n",
    "plt.plot(history[\"val_auc\"], 'g', label=\"Validation AUC\")\n",
    "\n",
    "# Plot settings\n",
    "plt.title(\"Model AUC\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"AUC\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `train_acc_history` and `val_acc_history` store accuracy per epoch\n",
    "plt.plot(history[\"precision\"], 'b', label=\"Train Precision\")\n",
    "plt.plot(history[\"val_precision\"], 'g', label=\"Validation Precision\")\n",
    "\n",
    "# Plot settings\n",
    "plt.title(\"Model Precision\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `train_acc_history` and `val_acc_history` store accuracy per epoch\n",
    "plt.plot(history[\"recall\"], 'b', label=\"Train Recall\")\n",
    "plt.plot(history[\"val_recall\"], 'g', label=\"Validation Recall\")\n",
    "\n",
    "# Plot settings\n",
    "plt.title(\"Model Recall\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Recall\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310_mohammad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
